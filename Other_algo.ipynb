{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ea6000",
   "metadata": {},
   "source": [
    "1. Tokenize our data using google's BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9b1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic imports\n",
    "\n",
    "# !pip install bert-for-tf2\n",
    "# !pip install sentencepiece\n",
    "\n",
    "import pandas as pd\n",
    "import bert\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb717b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if using colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c76a3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The absolute path to the directory containing all project files\n",
    "path = \"C:/Users/Marie/Organisation_Marie/X/3A/INF 554 - Machine Learning/Project/BERT/Clean/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our dataset into memory ; columns are : number, author, concatenation of abstracts, h index.\n",
    "raw_data = pd.read_csv(path+\"Abstracts Dataset.csv\")\n",
    "raw_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1855cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the abstracts : removing special characters, etc\n",
    "def preprocess_text(sen):\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen) # Remove punctuations and numbers\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence) # Single character removal\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence) # Removing multiple spaces, just in case\n",
    "    return sentence\n",
    "\n",
    "preprocessed_abstracts = []\n",
    "abstracts = list(raw_data['abstracts'])\n",
    "for sen in abstracts:\n",
    "    preprocessed_abstracts.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef90a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering the h indexes and the authors\n",
    "h_indexes = raw_data['hindex']\n",
    "authors = raw_data['authorID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05bc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the tokenizer object from the imports\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case) # Creating the tokenizer !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d19ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing our abstracts\n",
    "def tokenize_abstract(abstract):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(abstract))\n",
    "\n",
    "tokenized_abstracts = [tokenize_abstract(abstract) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the same but with the test set, for which we do not have the h indexes\n",
    "raw_test = pd.read_csv(path+\"Abstracts for test.csv\")\n",
    "raw_test.isnull().values.any()\n",
    "authors_test = raw_test[\"authorID\"]\n",
    "preprocessed_abstracts_test = []\n",
    "abstracts_test = list(raw_test['abstracts'])\n",
    "for sen in abstracts_test:\n",
    "    preprocessed_abstracts_test.append(preprocess_text(sen))\n",
    "tokenized_test_abstracts = [tokenize_abstract(abstract) for abstract in abstracts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ba970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnal : saving the tokenized versions into memory\n",
    "df = pd.DataFrame([[authors[i],tokenized_abstracts[i],h_indexes[i]] for i in range(len(h_indexes))], columns = [\"authorID\",\"tokenizedAbstract\",\"hindex\"])\n",
    "df.loc[:,[\"authorID\",\"tokenizedAbstract\",\"hindex\"]].to_csv(path+\"tokenizedDataset.csv\")\n",
    "df_test = pd.DataFrame([[authors_test[i],tokenized_test_abstracts[i]] for i in range(len(authors_test))], columns = [\"authorID\",\"tokenizedAbstract\"])\n",
    "df_test.loc[:,[\"authorID\",\"tokenizedAbstract\"]].to_csv(path+\"tokenizedTest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e751f4",
   "metadata": {},
   "source": [
    "2. Process our data and use it to fit a neuronal network using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "040148c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic imports\n",
    "\n",
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ea62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's batch our dataset\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "abstracts_with_len = [[abstract, h_indexes[i], len(abstract), authors[i]] for i, abstract in enumerate(tokenized_abstracts)]\n",
    "random.shuffle(abstracts_with_len)\n",
    "abstracts_with_len.sort(key=lambda x: x[2]) # Sorting according to the length of the (concatenation of) abstract\n",
    "for_fitting = [(abstract_lab[0], abstract_lab[1]) for abstract_lab in abstracts_with_len] #Keeping what is necessary for the NN fitting\n",
    "corresponding_authors = [abstracts_with_len[i][3] for i in range(len(abstracts_with_len))]\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: for_fitting, output_types=(tf.int32, tf.int32))\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "batched_dataset.shuffle(math.ceil(len(for_fitting) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, let's design our neuronal network. keep in mind that it is designed to return the h index of a tokenized concatenation of astracts.\n",
    "\n",
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,vocabulary_size,embedding_dimensions=128,cnn_filters=50,dnn_units=512,dropout_rate=0.1,training=False,name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        self.embedding = layers.Embedding(vocabulary_size,embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,kernel_size=2,padding=\"valid\",activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,kernel_size=3, padding=\"valid\",activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,kernel_size=4,padding=\"valid\",activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.last_dense = layers.Dense(units=1,activation=\"relu\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        return model_output\n",
    "\n",
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "DROPOUT_RATE = 0.2 # Between 0 and 1 ; the higher the value, the lesser the risk of overfitting\n",
    "NB_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of neuronal network and fitting it on our dataset\n",
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,embedding_dimensions=EMB_DIM,cnn_filters=CNN_FILTERS,dnn_units=DNN_UNITS,dropout_rate=DROPOUT_RATE)\n",
    "text_model.compile(loss=\"mean_squared_error\",optimizer=\"adam\",metrics=[\"mean_squared_error\"])\n",
    "text_model.fit(batched_dataset, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnal : saving our fitted neuronal network, for futur use\n",
    "text_model.save(\"NeuronalNetwork_\"+str(NB_EPOCHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f11786",
   "metadata": {},
   "source": [
    "3. Testing our fitted neuronal network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnal : loading an already trained neuronal network\n",
    "NN_name = \"NeuronalNetwork_\"+str(NB_EPOCHS)\n",
    "text_model = tf.keras.models.load_model(path+NN_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to batch our test set\n",
    "BATCH_SIZE = 64\n",
    "test_abstracts_with_len = [[abstract, len(abstract), test_authors[i]] for i, abstract in enumerate(tokenized_test_abstracts)]\n",
    "random.shuffle(test_abstracts_with_len)\n",
    "test_abstracts_with_len.sort(key=lambda x: x[1]) # Sorting according to the length of the (concatenation of) abstract\n",
    "for_testing = [(abstract_lab[0]) for abstract_lab in test_abstracts_with_len] #Keeping what is necessary for the NN testing\n",
    "corresponding_test_authors = [test_abstracts_with_len[i][2] for i in range(len(test_abstracts_with_len))]\n",
    "processed_testset = tf.data.Dataset.from_generator(lambda: for_testing, output_types=(tf.int32, tf.int32))\n",
    "batched_testset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668612ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the NN to predict our testset's h indexes\n",
    "res = text_model.predict(batched_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aeaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnal : saving the results for an ulterior submission\n",
    "df_res = pd.DataFrame([[corresponding_test_authors[i],res[i][0]] for i in range(len(corresponding_test_authors))],columns=[\"author\",\"hindex\"])\n",
    "df_res.loc[:,[\"author\",\"hindex\"]].to_csv(path+\"predictions_for_\"+NN_name+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb07585c",
   "metadata": {},
   "source": [
    "(Optionnal) 4. Getting into memory a pre-tokenized training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea218f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "df_train = pd.read_csv(path+\"tokenizedDataset.csv\")\n",
    "train_np = df_train.to_numpy()\n",
    "tokenized_abstracts = [train_np[i,2] for i in range(train_np.shape[0])]\n",
    "authors = [train_np[i,1] for i in range(train_np.shape[0])]\n",
    "h_indexes = [train_np[i,3] for i in range(train_np.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "df_test = pd.read_csv(path+\"tokenizedTest.csv\")\n",
    "test_np = df_test.to_numpy()\n",
    "tokenized_test_abstracts = [test_np[i,2] for i in range(test_np.shape[0])]\n",
    "authors_test = [test_np[i,1] for i in range(test_np.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f489c5",
   "metadata": {},
   "source": [
    "(Optionnal) 5. Generating the datasets with the given files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce2516b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 %\n",
      "100 % %\n",
      "100 % %\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bisect import bisect_left\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "purcent = 100\n",
    "path = \"C:/Users/Marie/Organisation_Marie/X/3A/INF 554 - Machine Learning/Project/BERT/Clean/\"\n",
    "\n",
    "# functions\n",
    "\n",
    "def interpret(abstracts,N,purcent=100):\n",
    "\n",
    "    def aux(i):\n",
    "        if i% purcent*200 == 0 :\n",
    "            print(str(int(100*100*i//(purcent*N)))+\" %\",end=\"\\r\")\n",
    "        return json.loads(abstracts[i][int(np.log10(paper_ids[i]))+5:])\n",
    "\n",
    "    itp_v = np.vectorize(aux)\n",
    "    content = itp_v(np.arange(0,purcent*N//100,1))\n",
    "    print(\"100 %\")\n",
    "    return content\n",
    "\n",
    "def get_h_indexes():\n",
    "    h_indexes = {}\n",
    "    file_h_index = open(train_path,'r')\n",
    "    reader = csv.reader(file_h_index)\n",
    "    i = 0\n",
    "    for row in reader :\n",
    "        if i != 0 :\n",
    "            h_indexes[row[0]]=row[1]\n",
    "        if i == 0 :\n",
    "            i = 1\n",
    "    return h_indexes\n",
    "\n",
    "def get_test_authors():\n",
    "    h_indexes = {}\n",
    "    file_h_index = open(test_path,'r')\n",
    "    reader = csv.reader(file_h_index)\n",
    "    i = 0\n",
    "    for row in reader :\n",
    "        if i != 0 :\n",
    "            h_indexes[row[1]]=0\n",
    "        if i == 0 :\n",
    "            i = 1\n",
    "    return h_indexes\n",
    "\n",
    "def get_papers_authors():\n",
    "    file_author_papers = open(author_papers_path,'r',encoding=\"utf-8\")\n",
    "    papers_authors = file_author_papers.readlines() # one line per author\n",
    "    papers = {}\n",
    "    for row in papers_authors :\n",
    "        row = row.rstrip('\\n')\n",
    "        temp = row.split(\":\")\n",
    "        if len(temp)!=2 :\n",
    "            print(\"error\")\n",
    "        author = temp[0]\n",
    "        papers_ = temp[1].split(\"-\")\n",
    "        papers[author]=papers_\n",
    "    return papers\n",
    "\n",
    "\n",
    "# getting everything in memory\n",
    "\n",
    "abstracts_path = path+\"abstracts.txt\"\n",
    "train_path = path+\"train.csv\"\n",
    "author_papers_path = path+\"author_papers.txt\"\n",
    "test_path = path+\"test.csv\"\n",
    "\n",
    "file = open(abstracts_path,'r',encoding='utf-8')\n",
    "\n",
    "abstracts = file.readlines() # list of all the \"abstracts\"\n",
    "N = len(abstracts) # number of papers\n",
    "\n",
    "paper_ids = np.array([int(abstracts[i].split(\"----\")[0]) for i in range(N)])\n",
    "content = interpret(abstracts,N,purcent)\n",
    "h_indexes = get_h_indexes()\n",
    "test = get_test_authors()\n",
    "papers = get_papers_authors()\n",
    "\n",
    "\n",
    "# reformatting train\n",
    "\n",
    "list_of_authors = list(h_indexes.keys())\n",
    "list_of_authors.sort()\n",
    "\n",
    "column1 = [ \"\" for _ in range(len(list_of_authors))]\n",
    "column2 = np.zeros(len(list_of_authors))\n",
    "\n",
    "for i in range(len(list_of_authors)):\n",
    "    \n",
    "    if i % 200 == 0 :\n",
    "        print(str(round(i*100/len(list_of_authors),2))+\" %\", end = \"\\r\")\n",
    "    \n",
    "    author_id = list_of_authors[i]\n",
    "    \n",
    "    concatenated_abstracts = \"\"\n",
    "    author_paper_ids = papers[str(author_id)]\n",
    "    \n",
    "    for j in range(len(author_paper_ids)):\n",
    "        \n",
    "        paper_id = int(author_paper_ids[j])\n",
    "        \n",
    "        index = bisect_left(paper_ids, paper_id)\n",
    "        \n",
    "        if index == paper_ids.size :\n",
    "            continue\n",
    "        \n",
    "        abstract_dico = content[index][\"InvertedIndex\"]\n",
    "        \n",
    "        numbers = [e for m in abstract_dico.values() for e in m]        \n",
    "        list_of_words = [\"\" for _ in range(max(numbers)+1)]\n",
    "        \n",
    "        for word in abstract_dico.keys():\n",
    "            for rank in abstract_dico[word]:\n",
    "                list_of_words[rank] = word+\" \"\n",
    "        \n",
    "        abstract = \"\".join(list_of_words)\n",
    "        \n",
    "        concatenated_abstracts = concatenated_abstracts + \" \"+abstract\n",
    "    \n",
    "    column1[i] = concatenated_abstracts\n",
    "    column2[i] = h_indexes[str(author_id)]\n",
    "\n",
    "print(\"100 %\")\n",
    "\n",
    "# reformatting test\n",
    "\n",
    "list_of_authors_test = list(test.keys())\n",
    "list_of_authors_test.sort()\n",
    "\n",
    "column3 = [ \"\" for _ in range(len(list_of_authors_test))]\n",
    "column4 = np.zeros(len(list_of_authors_test))\n",
    "\n",
    "for i in range(len(list_of_authors_test)):\n",
    "    \n",
    "    if i % 200 == 0 :\n",
    "        print(str(round(i*100/len(list_of_authors_test),2))+\" %\", end = \"\\r\")\n",
    "    \n",
    "    author_id = list_of_authors_test[i]\n",
    "    \n",
    "    concatenated_abstracts = \"\"\n",
    "    author_paper_ids = papers[str(author_id)]\n",
    "    \n",
    "    for j in range(len(author_paper_ids)):\n",
    "        \n",
    "        paper_id = int(author_paper_ids[j])\n",
    "        \n",
    "        index = bisect_left(paper_ids, paper_id)\n",
    "        \n",
    "        if index == paper_ids.size :\n",
    "            continue\n",
    "        \n",
    "        abstract_dico = content[index][\"InvertedIndex\"]\n",
    "        \n",
    "        numbers = [e for m in abstract_dico.values() for e in m]        \n",
    "        list_of_words = [\"\" for _ in range(max(numbers)+1)]\n",
    "        \n",
    "        for word in abstract_dico.keys():\n",
    "            for rank in abstract_dico[word]:\n",
    "                list_of_words[rank] = word+\" \"\n",
    "        \n",
    "        abstract = \"\".join(list_of_words)\n",
    "        \n",
    "        concatenated_abstracts = concatenated_abstracts + \" \"+abstract\n",
    "    \n",
    "    column3[i] = concatenated_abstracts\n",
    "\n",
    "print(\"100 %\")\n",
    "\n",
    "# saving everything on harddrive\n",
    "\n",
    "results_path_train = path+\"Abstracts Dataset.csv\"\n",
    "results_path_test = path+\"Abstracts for test.csv\"\n",
    "\n",
    "def serialize(column0,column1,column2,file_path):\n",
    "    agg = [[column0[i],column1[i],column2[i]] for i in range(len(column1))]\n",
    "    df = pd.DataFrame(agg,columns=[\"authorID\",\"abstracts\",\"hindex\"])\n",
    "    df.loc[:,[\"authorID\",\"abstracts\",\"hindex\"]].to_csv(file_path)\n",
    "    \n",
    "serialize(list_of_authors,column1,column2,results_path_train)\n",
    "serialize(list_of_authors_test,column3,column4,results_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289933b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
