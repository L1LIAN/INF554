{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb102ac",
   "metadata": {},
   "source": [
    "Our main idea here is to understand how words are linked to their user's h indexes.\n",
    "Ideally, we would like to find words that are mostly used by scientists with very high h indexes, so that if an author of the test set uses them, we can infer that their h indexes must be high as well. However, we also need such words to be used by enough scientists, otherwise the probability of a scientist in the test set using it is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52042023",
   "metadata": {},
   "source": [
    "1. Getting everything into memory and building a dictionnary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124d3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from bisect import bisect_left\n",
    "import csv\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc54d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "\n",
    "purcent = 100 # pourcentage of analysed data from abstracts.txt\n",
    "author_threshold = 3 # minimal number of authors having used the word\n",
    "word_length_threshold = 3 # minimal length of a word (shorter words are discarded)\n",
    "\n",
    "path = \"C:/Users/Marie/Organisation_Marie/X/3A/INF 554 - Machine Learning/Project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting into memory the different parts of the dataset\n",
    "\n",
    "def interpret(abstracts,N,purcent=100):\n",
    "    def aux(i):\n",
    "        if i% purcent*200 == 0 :\n",
    "            print(str(int(100*100*i//(purcent*N)))+\" %\",end=\"\\r\")\n",
    "        return json.loads(abstracts[i][int(numpy.log10(paper_ids[i]))+5:])\n",
    "    itp_v = numpy.vectorize(aux)\n",
    "    content = itp_v(numpy.arange(0,purcent*N//100,1))\n",
    "    return content\n",
    "\n",
    "def get_content_and_paper_ids():\n",
    "    file_abstracts = open(path+\"abstracts.txt\",'r',encoding='utf-8')\n",
    "\n",
    "    abstracts = file_abstracts.readlines() # list of all the \"abstracts\"\n",
    "    file_abstracts.close()\n",
    "    N = len(abstracts) # total number of abstracts ; equals 624,181\n",
    "    paper_ids = numpy.array([int(abstracts[i].split(\"----\")[0]) for i in range(N)]) # the list of the abstract's IDs (already sorted)\n",
    "    content = interpret(abstracts,N,purcent) # The list of abstracts as dictionnaries\n",
    "    \n",
    "    return paper_ids,content\n",
    "\n",
    "def get_h_indexes():\n",
    "    h_indexes = {}\n",
    "    file_h_index = open(path+\"train.csv\",'r')\n",
    "    reader = csv.reader(file_h_index)\n",
    "    i = 0\n",
    "    for row in reader :\n",
    "        if i != 0 :\n",
    "            h_indexes[row[0]]=row[1]\n",
    "        if i == 0 :\n",
    "            i = 1\n",
    "    return h_indexes\n",
    "\n",
    "def get_papers_authors():\n",
    "    file_author_papers = open(path+\"author_papers.txt\",'r',encoding=\"utf-8\")\n",
    "    papers_authors = file_author_papers.readlines() # one line per author\n",
    "    papers = {}\n",
    "    for row in papers_authors :\n",
    "        row = row.rstrip('\\n')\n",
    "        temp = row.split(\":\")\n",
    "        if len(temp)!=2 :\n",
    "            print(\"error\")\n",
    "        author = temp[0]\n",
    "        papers_ = temp[1].split(\"-\")\n",
    "        papers[author]=papers_\n",
    "    return papers\n",
    "\n",
    "def get_reds():\n",
    "    reds = []\n",
    "    file_reds = open(path+\"test.csv\",'r')\n",
    "    reader = csv.reader(file_reds)\n",
    "    i = 0\n",
    "    for row in reader :\n",
    "        if i != 0 :\n",
    "            reds.append(row[1])\n",
    "        if i == 0 :\n",
    "            i = 1\n",
    "    return reds\n",
    "\n",
    "paper_ids,content = get_content_and_paper_ids()\n",
    "h_indexes = get_h_indexes()\n",
    "papers = get_papers_authors()\n",
    "reds = get_reds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's make our dictionnary of words, as a python sorted list.\n",
    "\n",
    "def dictionnaire(content,N,purcent=100):\n",
    "    words = [\"\"]\n",
    "    for i in range(purcent*N//100):\n",
    "        if i% purcent*200 == 0 :\n",
    "            print(str(int(100*100*i//(purcent*N)))+\" %\",end=\"\\r\")\n",
    "        dico = content[i]\n",
    "        keys = list(dico[\"InvertedIndex\"].keys())\n",
    "        for word in keys :\n",
    "            word = re.sub(r\"[^a-zA-Z]\", \"\", word) # preprocessing the word\n",
    "            word = word.lower()\n",
    "            if len(word) > word_length_threshold :\n",
    "                index = bisect_left(words, word)\n",
    "                if index == len(words) or words[index] > word or words[index] < word :\n",
    "                    words.insert(index,word)\n",
    "    return words\n",
    "\n",
    "words = dictionnaire(content,len(content),purcent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52c7f5",
   "metadata": {},
   "source": [
    "2. Computing which authors (+ corresponding h index is we know it) used which word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correspondance(words,paper_ids,content,h_indexes,papers):\n",
    "\n",
    "    super_object = [[] for i in range(len(words))]\n",
    "\n",
    "    authors = list(h_indexes.keys())\n",
    "    J = len(authors)\n",
    "    j = 0\n",
    "    for author in authors :\n",
    "        if j % (J//1000) == 0 :\n",
    "            print(round(100*j/J,1),end=\"\\r\")\n",
    "        h = h_indexes[author]\n",
    "        for paper_id in papers[author]:\n",
    "            paper_id = int(paper_id)\n",
    "            index_in_content = bisect_left(paper_ids, paper_id)\n",
    "            if index_in_content < len(content):\n",
    "                paper_content = content[index_in_content][\"InvertedIndex\"].keys() #loss of info : how many time the word was used\n",
    "                for word in paper_content :\n",
    "                    word = re.sub(r\"[^a-zA-Z]\", \"\", word) # preprocessing the word\n",
    "                    word = word.lower()\n",
    "                    index = bisect_left(words, word)\n",
    "                    if index < len(words) and words[index]==word :\n",
    "                        super_object[index].append([author,paper_id,int(float(h))])\n",
    "        j += 1\n",
    "    \n",
    "    for red in reds :\n",
    "        for paper_id in papers[red]:\n",
    "            paper_id = int(paper_id)\n",
    "            index_in_content = bisect_left(paper_ids, paper_id)\n",
    "            if index_in_content < len(content):\n",
    "                paper_content = content[index_in_content][\"InvertedIndex\"].keys() #loss of info : how many time the word was used\n",
    "                for word in paper_content :\n",
    "                    word = re.sub(r\"[^a-zA-Z]\", \"\", word) # preprocessing the word\n",
    "                    word = word.lower()\n",
    "                    index = bisect_left(words, word)\n",
    "                    if index < len(words) and words[index]==word :\n",
    "                        super_object[index].append([red,paper_id,None)])\n",
    "    return super_object\n",
    "\n",
    "correspondance = get_correspondance(words,paper_ids,content,h_indexes,papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.b) let's see what correspondance looks like\n",
    "\n",
    "def create_eff_means(correspondance):\n",
    "    W = len(correspondance)\n",
    "    means = numpy.zeros(W)\n",
    "    eff = numpy.zeros(W)\n",
    "    i = 0\n",
    "    for i in range(W) :\n",
    "        eff[i] = len(correspondance[i])\n",
    "        if eff[i] != 0 :\n",
    "            means[i] = sum(correspondance[i])/eff[i]\n",
    "        else :\n",
    "            means[i] = -1\n",
    "    return eff, means\n",
    "\n",
    "effectifs,means = create_eff_means(correspondance)\n",
    "# most used word ?\n",
    "print(words[numpy.argmax(effectifs)])\n",
    "# proportion of words used in less than 5 papers ?\n",
    "print(round(100*effectifs[<5].size()/len(words),1),\" %\")\n",
    "# how many words are used in more than 10% of the papers ?\n",
    "print(effectifs[>len(words)/10].size())\n",
    "# what word has the highest \"mean h index\" ?\n",
    "print(words[numpy.argmax(means)])\n",
    "\n",
    "# We could even draw some diagrams !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnal : saving our results\n",
    "results = [[words[i],correspondance[i],means[i],eff[i]] for i in range(len(words))]\n",
    "dataframe = pd.DataFrame(results,columns=[\"word\",\"correspondance\",\"means\",\"effectifs\"])\n",
    "dataframe.to_csv(path+\"AnalysisOfAbstracts_results_\"+str(purcent)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7c259",
   "metadata": {},
   "source": [
    "3. Making a predictor based on our discoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe5dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(authorID,correspondance):\n",
    "    neighbour_h = []\n",
    "    for i in range(len(correspondance)):\n",
    "        indic = False\n",
    "        for match in correspondance[i]:\n",
    "            if match[0]==authorID :\n",
    "                indic = True\n",
    "        if indic :\n",
    "            for match in correspondance[i]:\n",
    "                h = match[2]\n",
    "                if h != None : \n",
    "                    neighbour_h.append(h)\n",
    "    return func(neighbour_h)\n",
    "    \n",
    "def func(neighbour_h):\n",
    "    # Here we have a lot of possibilities ! we could even do some learning to compute a good function.\n",
    "    # First, let's try with a classic : the mean\n",
    "    return numpy.mean(numpy.array(neighbour_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try our predictor !\n",
    "predictions = [[authorID,predict(authorID)] for authorID in reds]\n",
    "df_pred = pd.DataFrame(predictions,columns=[\"author\",\"hindex\"])\n",
    "df_pred.to_csv(path+\"for_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
